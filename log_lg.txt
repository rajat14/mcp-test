{"timestamp": "2025-09-20 07:52:56", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:52", "message": "APP LOG: Inference params: {'temperature': 0, 'max_new_tokens': 4096, 'top_p': 1, 'n': 1, 'model': 'exl-isc-minerva-openai-rg02-svc-t03'}"}
{"timestamp": "2025-09-20 07:52:56", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:53", "message": "APP LOG: Prompt: Instruction: You are the Intelligent Planner for a conversational AI agent. Your job is to decide the NEXT BEST ACTION based on the user's original query and the results of any previously executed tools.\nTool Access:\nAvailable tools:\n- Health: API endpoint: GET /health (params: {'query_params': [], 'request_body': []})\n- Insight: Generates Insights for a given namespace.table_name.column (we call this a nodeId)combination. It queries lineage for a given nodeId and generates upstream and downstream insights based on the lineage graph.  (params: {'query_params': [], 'request_body': ['namespace', 'table_name', 'column_name', 'insight']})\n- Summarize And Format Responses: responds and formats multiple api responses into a single coherent response. It takes a user query and additional information as input and generates a summarized response.The additional information can be a sql query and/or data and/or json and/or other relevant information. (params: {'query_params': [], 'request_body': ['user_query', 'additional_information', 'response']})\n- Execute With Langgraph: API endpoint: POST /langgraph/execute (params: {'query_params': [], 'request_body': ['user_query']})\n- Execute Custom Workflow: API endpoint: POST /langgraph/workflow/custom (params: {'query_params': [], 'request_body': []})\n- Post Lineage: Extracts lineage information from uploaded files and stores it in a vectorDB. Supports various file types such as SAS, Spark, and SQL. This endpoint does not return the lineage data directly; instead, it processes the file and stores the extracted lineage information for future queries. (params: {'query_params': [], 'request_body': ['workflow_name', 'workflow_id']})\n- Lineage Status: Retrieves the status of a previously submitted lineage extraction workflow. This endpoint allows users to check whether the lineage extraction process has completed successfully, is still in progress, or has encountered any errors. (params: {'query_params': [], 'request_body': ['workflow_name', 'status']})\n- Get Lineage: Fetches lineage information for a specified dataset or column within a given namespace. Users can specify whether they want to retrieve upstream and/or downstream lineage, as well as the depth of the lineage graph. This endpoint is useful for understanding data dependencies and the flow of data within an organization. (params: {'query_params': [], 'request_body': ['lineage']})\n- Smart Data Retrieval: retrieves similar database.table_name.column_name combinations based on a user query.  Use this api when you have a database/table/column in natural language and you want to find the actual column names. This api does not provide any additional information about the columns like data type, description etc. It only provides the database, table and column names. (params: {'query_params': [], 'request_body': ['user_query', 'results']})\n- Text To Sql: This api generates SQL query over openlineage data model given a natural language input. This generated SQL query can then be executed using the /text_to_sql/execute api. (params: {'query_params': [], 'request_body': ['user_query', 'sql']})\n- Execute Sql: Execute a SQL query and return the actual data results. Use this AFTER generating SQL with text_to_sql. (params: {'query_params': [], 'request_body': ['sql_query', 'result']})\nContext:\nOriginal User Query: \"how many tools do you have?.\"\nExecution History (Results from previous steps):\n[]\nTask:\nDetermine the single best action to take now.\nRules for Action:\n * If the query is fully answered by the history OR if the query is a simple greeting/internal question: Choose respond.\n * If a tool is needed: Choose call_tool. This applies to the first tool in a sequence (e.g., calling Text To Sql) or the next tool after a previous execution finished (e.g., calling Execute Sql after Text To Sql succeeded).\nSpecial Rule for Database Queries (Text To Sql to Execute Sql):\n * If the user's goal is to get data, your FIRST action must be to call the Text To Sql tool.\n * If the last step in the history was a successful call to Text To Sql, your NEXT action MUST be to call Execute Sql, using the generated SQL from the history as a parameter.\nRespond ONLY with a JSON object in one of the two formats below:\n\nFormat 1: Call a Tool\n{\n    \"action\": \"call_tool\",\n    \"reasoning\": \"Explain why this specific tool is necessary now, considering the history.\",\n    \"tool\": \"[tool name from Available tools]\",\n    \"parameters\": {\n        \"request_body\": {\n            \"key\": \"value\",\n            \"sql_query\": \"Use this key to pass SQL if calling Execute Sql\"\n        }\n    }\n}\n\nFormat 2: Respond to User\n{\n    \"action\": \"respond\",\n    \"reasoning\": \"Explain that the answer is ready or no tool is needed.\",\n    \"response\": \"Provide a brief, natural language draft of the final answer or next question to ask the user.\"\n}\n\n"}
{"timestamp": "2025-09-20 07:52:56", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:54", "message": "APP LOG: Response: {\n    \"action\": \"respond\",\n    \"reasoning\": \"The user's query is a simple internal question asking about the number of tools available. This can be answered directly without the need for any tool.\",\n    \"response\": \"We have a total of 11 tools available for various tasks including health checks, generating insights, summarizing responses, executing workflows, extracting and retrieving lineage information, smart data retrieval, and SQL query generation and execution.\"\n}"}
{"timestamp": "2025-09-20 07:52:56", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\langgraph\\nodes.py:94", "message": "APP LOG: Synthesizer Node: Generating final response."}
{"timestamp": "2025-09-20 07:52:56", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\decorators\\log_entry_exit.py:30", "message": "APP LOG: Executing function: query_via_config"}
{"timestamp": "2025-09-20 07:52:58", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:52", "message": "APP LOG: Inference params: {'temperature': 0, 'max_new_tokens': 4096, 'top_p': 1, 'n': 1, 'model': 'exl-isc-minerva-openai-rg02-svc-t03'}"}
{"timestamp": "2025-09-20 07:52:58", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:53", "message": "APP LOG: Prompt: Original user query: \"how many tools do you have?.\"\nExecution plan: {\"action\": \"synthesize_final\"}\nResults from tools: {\"history\": []}\n\nCreate a natural response that:\n1. Directly answers the user's original question given the results\n2. Synthesizes information from multiple tool results if applicable\n3. Do not explain the execution plan or tool details unless relevant\n\nGuidelines:\n- Start with the answer to their question\n- If you are unable to answer the questions because there were errors, explain that to the user clearly and suggest next steps\n- Keep it concise but informative\n\nRespond as if you're a helpful assistant explaining the results. Use markdown formatting if needed."}
{"timestamp": "2025-09-20 07:52:58", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:54", "message": "APP LOG: Response: I currently don't have any tools available to provide a specific answer to your question. If you need information about a particular type of tool or service, please let me know, and I'll do my best to assist you!"}
{"timestamp": "2025-09-20 07:55:23", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\langgraph\\nodes.py:15", "message": "APP LOG: Planner Node: Starting new planning cycle."}
{"timestamp": "2025-09-20 07:55:23", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\decorators\\log_entry_exit.py:30", "message": "APP LOG: Executing function: query_via_config"}
{"timestamp": "2025-09-20 07:55:26", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:52", "message": "APP LOG: Inference params: {'temperature': 0, 'max_new_tokens': 4096, 'top_p': 1, 'n': 1, 'model': 'exl-isc-minerva-openai-rg02-svc-t03'}"}
{"timestamp": "2025-09-20 07:55:26", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:53", "message": "APP LOG: Prompt: Instruction: You are the Intelligent Planner for a conversational AI agent. Your job is to decide the NEXT BEST ACTION based on the user's original query and the results of any previously executed tools.\nTool Access:\nAvailable tools:\n- Health: API endpoint: GET /health (params: {'query_params': [], 'request_body': []})\n- Insight: Generates Insights for a given namespace.table_name.column (we call this a nodeId)combination. It queries lineage for a given nodeId and generates upstream and downstream insights based on the lineage graph.  (params: {'query_params': [], 'request_body': ['namespace', 'table_name', 'column_name', 'insight']})\n- Summarize And Format Responses: responds and formats multiple api responses into a single coherent response. It takes a user query and additional information as input and generates a summarized response.The additional information can be a sql query and/or data and/or json and/or other relevant information. (params: {'query_params': [], 'request_body': ['user_query', 'additional_information', 'response']})\n- Execute With Langgraph: API endpoint: POST /langgraph/execute (params: {'query_params': [], 'request_body': ['user_query']})\n- Execute Custom Workflow: API endpoint: POST /langgraph/workflow/custom (params: {'query_params': [], 'request_body': []})\n- Post Lineage: Extracts lineage information from uploaded files and stores it in a vectorDB. Supports various file types such as SAS, Spark, and SQL. This endpoint does not return the lineage data directly; instead, it processes the file and stores the extracted lineage information for future queries. (params: {'query_params': [], 'request_body': ['workflow_name', 'workflow_id']})\n- Lineage Status: Retrieves the status of a previously submitted lineage extraction workflow. This endpoint allows users to check whether the lineage extraction process has completed successfully, is still in progress, or has encountered any errors. (params: {'query_params': [], 'request_body': ['workflow_name', 'status']})\n- Get Lineage: Fetches lineage information for a specified dataset or column within a given namespace. Users can specify whether they want to retrieve upstream and/or downstream lineage, as well as the depth of the lineage graph. This endpoint is useful for understanding data dependencies and the flow of data within an organization. (params: {'query_params': [], 'request_body': ['lineage']})\n- Smart Data Retrieval: retrieves similar database.table_name.column_name combinations based on a user query.  Use this api when you have a database/table/column in natural language and you want to find the actual column names. This api does not provide any additional information about the columns like data type, description etc. It only provides the database, table and column names. (params: {'query_params': [], 'request_body': ['user_query', 'results']})\n- Text To Sql: This api generates SQL query over openlineage data model given a natural language input. This generated SQL query can then be executed using the /text_to_sql/execute api. (params: {'query_params': [], 'request_body': ['user_query', 'sql']})\n- Execute Sql: Execute a SQL query and return the actual data results. Use this AFTER generating SQL with text_to_sql. (params: {'query_params': [], 'request_body': ['sql_query', 'result']})\nContext:\nOriginal User Query: \"can you find the column lineage of lgd_pit column?\"\nExecution History (Results from previous steps):\n[]\nTask:\nDetermine the single best action to take now.\nRules for Action:\n * If the query is fully answered by the history OR if the query is a simple greeting/internal question: Choose respond.\n * If a tool is needed: Choose call_tool. This applies to the first tool in a sequence (e.g., calling Text To Sql) or the next tool after a previous execution finished (e.g., calling Execute Sql after Text To Sql succeeded).\nSpecial Rule for Database Queries (Text To Sql to Execute Sql):\n * If the user's goal is to get data, your FIRST action must be to call the Text To Sql tool.\n * If the last step in the history was a successful call to Text To Sql, your NEXT action MUST be to call Execute Sql, using the generated SQL from the history as a parameter.\nRespond ONLY with a JSON object in one of the two formats below:\n\nFormat 1: Call a Tool\n{\n    \"action\": \"call_tool\",\n    \"reasoning\": \"Explain why this specific tool is necessary now, considering the history.\",\n    \"tool\": \"[tool name from Available tools]\",\n    \"parameters\": {\n        \"request_body\": {\n            \"key\": \"value\",\n            \"sql_query\": \"Use this key to pass SQL if calling Execute Sql\"\n        }\n    }\n}\n\nFormat 2: Respond to User\n{\n    \"action\": \"respond\",\n    \"reasoning\": \"Explain that the answer is ready or no tool is needed.\",\n    \"response\": \"Provide a brief, natural language draft of the final answer or next question to ask the user.\"\n}\n\n"}
{"timestamp": "2025-09-20 07:55:26", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:54", "message": "APP LOG: Response: {\n    \"action\": \"call_tool\",\n    \"reasoning\": \"To find the column lineage of the 'lgd_pit' column, we need to fetch the lineage information for this specific column. The 'Get Lineage' tool is appropriate for this task.\",\n    \"tool\": \"Get Lineage\",\n    \"parameters\": {\n        \"request_body\": {\n            \"lineage\": {\n                \"namespace\": \"default\",\n                \"table_name\": \"your_table_name\",  // Replace with the actual table name if known\n                \"column_name\": \"lgd_pit\",\n                \"depth\": 1,  // Adjust depth as needed\n                \"direction\": \"both\"  // Fetch both upstream and downstream lineage\n            }\n        }\n    }\n}"}
{"timestamp": "2025-09-20 07:55:26", "level": "ERROR", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\langgraph\\nodes.py:43", "message": "APP LOG: Planner Node Error: Expecting property name enclosed in double quotes: line 9 column 51 (char 409)"}
{"timestamp": "2025-09-20 07:55:26", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\langgraph\\nodes.py:94", "message": "APP LOG: Synthesizer Node: Generating final response."}
{"timestamp": "2025-09-20 07:55:26", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\decorators\\log_entry_exit.py:30", "message": "APP LOG: Executing function: query_via_config"}
{"timestamp": "2025-09-20 07:55:30", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:52", "message": "APP LOG: Inference params: {'temperature': 0, 'max_new_tokens': 4096, 'top_p': 1, 'n': 1, 'model': 'exl-isc-minerva-openai-rg02-svc-t03'}"}
{"timestamp": "2025-09-20 07:55:30", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:53", "message": "APP LOG: Prompt: Original user query: \"can you find the column lineage of lgd_pit column?\"\nExecution plan: {\"action\": \"synthesize_final\"}\nResults from tools: {\"history\": []}\n\nCreate a natural response that:\n1. Directly answers the user's original question given the results\n2. Synthesizes information from multiple tool results if applicable\n3. Do not explain the execution plan or tool details unless relevant\n\nGuidelines:\n- Start with the answer to their question\n- If you are unable to answer the questions because there were errors, explain that to the user clearly and suggest next steps\n- Keep it concise but informative\n\nRespond as if you're a helpful assistant explaining the results. Use markdown formatting if needed."}
{"timestamp": "2025-09-20 07:55:30", "level": "INFO", "transaction_id": "", "logger_name": "app_logger", "location": "src\\services\\inference\\inference_service.py:54", "message": "APP LOG: Response: To find the column lineage of the `lgd_pit` column, you would typically need to trace back through the data transformations and sources that contribute to the final value of this column. This involves understanding the data flow, including any calculations, joins, or aggregations that lead to the creation of `lgd_pit`.\n\nUnfortunately, I don't have the specific lineage details for the `lgd_pit` column in your dataset. To obtain this information, you can:\n\n1. **Check the Data Documentation**: Review any available data dictionaries, ETL (Extract, Transform, Load) process documentation, or data lineage tools that your organization uses.\n2. **Examine the ETL Code**: Look at the scripts or workflows that generate the `lgd_pit` column. This might involve SQL queries, Python scripts, or other data processing tools.\n3. **Consult with Data Engineers**: If the documentation is not clear, speaking with the data engineers or analysts who manage the data pipeline can provide insights into how the `lgd_pit` column is derived.\n\nIf you need further assistance, please provide more context or specific details about your data environment, and I can guide you on the next steps."}
{"timestamp": "2025-09-20 07:56:42", "level": "INFO", "transaction_id": "8ec15acb-3d5b-4166-aee4-516f7fb0885f", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\decorators\\transaction_data.py:22", "message": "APP LOG: Transaction data: transaction_id='8ec15acb-3d5b-4166-aee4-516f7fb0885f' user_id=None custom_data=None"}
{"timestamp": "2025-09-20 07:56:42", "level": "INFO", "transaction_id": "8ec15acb-3d5b-4166-aee4-516f7fb0885f", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\logger\\logger.py:109", "message": "APP LOG: Entering api router: get_available_tools"}
{"timestamp": "2025-09-20 07:56:42", "level": "INFO", "transaction_id": "8ec15acb-3d5b-4166-aee4-516f7fb0885f", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\logger\\logger.py:115", "message": "APP LOG: Exiting api router: get_available_tools"}
{"timestamp": "2025-09-20 07:56:42", "level": "INFO", "transaction_id": "8ec15acb-3d5b-4166-aee4-516f7fb0885f", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\performance\\execution_stats.py:298", "message": "APP LOG: src.web.routers.mcp.get_available_tools execution time: 0.006778717041015625"}
{"timestamp": "2025-09-20 07:56:42", "level": "INFO", "transaction_id": "8ec15acb-3d5b-4166-aee4-516f7fb0885f", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\performance\\execution_stats.py:153", "message": "APP LOG: \nSTART REPORT: Performance stats\nTotal execution time (seconds): 0.01\n#\tFunction\tExecuted at\tExecuted at (unix)\tExecution time (seconds)\tPercentage of total execution time\tNotes\n8ec15acb-3d5b-4166-aee4-516f7fb0885f\tsrc.web.routers.mcp.get_available_tools\t2025-09-20 07:56:42\t1758355002.929675\t0.01\t75.32%\tNone\nNote that function calls that account for approximately 0% of the execution time are not included in the report\nEND REPORT: Performance stats\n"}
{"timestamp": "2025-09-20 07:56:42", "level": "INFO", "transaction_id": "8ec15acb-3d5b-4166-aee4-516f7fb0885f", "logger_name": "app_logger", "location": "src\\core\\instrumentation\\performance\\execution_stats.py:298", "message": "APP LOG: src.web.routers.mcp.get_available_tools execution time: 0.030917882919311523"}
